SentinelX: Detailed Software Summary

The software architecture for SentinelX: AI-Powered Smart Surveillance Drone is designed to seamlessly integrate simulation and hardware implementation, ensuring robust performance and scalability. Below is a detailed breakdown of the software stack and its functionality in both simulation and hardware stages.


---

1. Simulation Phase

The simulation phase uses Gazebo and ROS2 to replicate the drone's behavior and environment, enabling validation of AI models, control systems, and flight behaviors before transitioning to physical hardware.

Key Components

1. Gazebo Simulator:

Provides a realistic 3D environment to simulate drone flight dynamics, sensor behavior, and environmental conditions.

Integrates with PX4 SITL (Software-in-the-Loop) for accurate flight physics and control.

Includes NPCs (Non-Playable Characters) such as humans, vehicles, and obstacles to simulate real-world scenarios (e.g., surveillance, hazard detection).



2. ROS2 Middleware:

Serves as the backbone for communication between modules.

Nodes manage flight control, AI processing, sensor data handling, and simulation interactions.



3. AI Integration:

YOLOv8 and NVIDIA DeepStream for object detection.

Custom AI models for fire, bomb, and weapon detection, implemented via ROS2 nodes.

Slicing Aided Hyper Inference (SAHI) for better small-object detection.

Models are tested in Gazebo with synthetic data generated by NPCs.



4. Control Logic:

Manual Control Simulation: Allows for user-controlled navigation via a virtual joystick or MAVLink commands.

Autonomous Modes: Includes patrol, object tracking, stationary monitoring, and descent-based analysis, implemented using waypoints and AI triggers.



5. Testing Pipeline:

Perform stress testing for AI inference under various environmental conditions.

Validate surveillance modes (e.g., tracking NPCs, detecting hazards) in a controlled, reproducible environment.



6. Data Visualization:

Real-time telemetry and video feeds displayed on a dashboard using ROS2-compatible visualization tools like Rviz.

AI-detected objects and threat reports are overlaid on the live feed.





---

2. Hardware Implementation Phase

The hardware implementation phase connects the software stack to the physical drone, incorporating manual and autonomous controls, advanced AI processing, and real-time data sharing. This phase transitions the simulation logic into real-world applications.

Key Components

Flight Controller Integration

Flight Controller: Pixhawk 6X, running PX4 firmware, handles core flight operations (e.g., stabilization, navigation).

Dual Communication System:

Radio Transmitter/Receiver: Enables manual control via a ground control station or remote controller.

NVIDIA Jetson Orin Nano: Acts as the companion computer for AI-driven automatic control and real-time decision-making.



Companion Computer (Jetson Orin Nano)

1. AI Processing:

Onboard Inference: NVIDIA TensorRT and DeepStream SDK enable efficient edge-based AI inference.

Handles YOLO-based object detection, hazard identification, and anomaly triggers.



2. ROS2 Nodes:

AI Node: Processes video feeds for object detection and sends alerts to the flight controller.

Control Node: Interfaces with the Pixhawk flight controller via MAVLink for autonomous navigation.



3. Control Modes:

Manual Mode: User-controlled flight using the radio transmitter, bypassing AI logic.

Automatic Mode: Jetson Orin Nano processes sensor inputs and autonomously adjusts flight patterns.



4. Data Handling:

Sensor data (e.g., camera feeds, thermal imaging) is processed locally and transmitted to a ground control station or cloud via Wi-Fi/4G/5G.

Encrypted data sharing ensures secure communication with authorities for real-time reporting.




Sensor Integration

High-Resolution Camera: Captures live video for AI processing and FPV transmission.

Thermal Camera: Used for fire detection and low-visibility operations.

LIDAR/Ultrasonic Sensors: Enables obstacle avoidance and precise landing.

GPS Module: Provides geolocation data for autonomous navigation and reporting.



---

Unified Features Across Simulation and Hardware

1. Surveillance Modes:

Patrolling, object tracking, stationary monitoring, and descent-based analysis.



2. AI-Driven Automation:

Real-time object detection and decision-making.

Trigger actions (e.g., descending or stopping) based on detected threats.



3. Real-Time Visualization:

Dashboards display telemetry, live video feeds, and AI detections in both simulation and hardware setups.



4. Data Transmission:

Instant analysis reports are shared with authorities in secure formats.



5. Flexibility:

Modular architecture supports switching between onboard and base station processing.





---

By combining advanced simulation tools and hardware integration, the SentinelX drone ensures a smooth transition from concept to deployment, maximizing reliability, efficiency, and performance.